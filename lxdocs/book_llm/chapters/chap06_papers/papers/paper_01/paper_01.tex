\documentclass[../../../../main.tex]{subfiles}
\begin{document}


% \input{\subfix{images/Embedding_Models.tex}}
% \input{\subfix{tables/context_window.tex}}

\chapter{MCQGen: A Large Language Model-Driven MCQ Generator for Personalized Learning} 
	
	\section{Information }
	
	\begin{enumerate}
		\item \cite{Hang2024}
    \item IEEE Access , DOI : 10.1109/ACCESS.2024.3420709
		\item \url{https://drive.google.com/file/d/1KkfsKJhbx3S2hLeA6iMZsXpkQdwaRPMb/view?usp=drive_link}
	\end{enumerate}

	\section{Summarize this paper in aimed at a beginner school student. Cover these aspects: the problem the paper addresses, why this problem matters, what the authors did to solve it, how they tested or evaluated their work, and the main result or finding.}
	
	\begin{enumerate}
    \item They built a system called MCQGen to make MCQ (Multiple Choice Questions) for tests and practice.
    \item personalized learning, where each student gets questions that fit their understanding.
  \end{enumerate}

  \clearpage

  \section{What is the the initial input and final output of the work propesed by this paper. aimed at a beginner school student.}
	
	\begin{enumerate}
    \item Input : A paragraph or text that the questions should be based on.
    \item Output: MCQ questions
    \begin{enumerate}
      \item question stem
      \item correct answer
      \item three or four wrong options (distractors)
      \item Difficulty level
      \item Bloom’s Taxonomy level
    \end{enumerate}
  \end{enumerate}

  \section{What the pipeline stages which converts input to final output. Give a step-by-step narrative explanation of each stage (like how data flows and decisions happen)? your answer should be aimed at a beginner school student. Just write the names of the stages we will discuss each stage later.}
  \begin{enumerate}
    \item Input text collection
    \item Preprocessing / cleaning
    \item Important-sentence / concept extraction
    \item Content selection and target spotting
    \item Prompt construction for the LLM
    \item LLM generation (question + correct answer + distractors)
    \item Postprocessing and automatic quality checks
    \item Difficulty estimation and Bloom’s level classification
    \item Human review / validation (optional)
    \item Formatting and final output delivery
  \end{enumerate}

  \clearpage
  \section{Make a table with 5 columns: stage name, input to this stage, description, tools used, outputs of this stage. your answer should be aimed at a beginner school student.write latex code. the table should not be truncated in page. all collumns should be visibel.}
    \input{\subfix{table_02.tex}}

  \clearpage
  \section{Stage 1 - Input \& Settings }
  \begin{enumerate}
    \item \textbf{Input to this stage:} educational text and simple settings (topic, difficulty level, grade, number of questions).
    \item \textbf{Output of this stage:} a set of clean, readable text chunks.
    \item \textbf{What happens (simple):} System breaks input text into clean small chunks if the text is large.
    \item \textbf{Tools used:} A web form or file upload in the system; no heavy AI yet.
  \end{enumerate}


  \section{Stage 2 - Preprocessing \& Text Cleaning}
    \begin{enumerate}
      \item \textbf{Input to this stage:} clean text chunks from Stage 1.
      \item \textbf{Output of this stage:} well-formatted text ready for sentence extraction.
      \item \textbf{What happens (simple):} system removes extra spaces, strange symbols, repeated lines, and makes sure sentences end properly.
      \item \textbf{Tools used:} basic text-cleaning scripts (no AI), simple Python string functions.
    \end{enumerate}

    \section{Stage 3 - Important Sentence Extraction}
      \begin{enumerate}
        \item \textbf{Input to this stage:} cleaned text chunks from Stage 2.
        \item \textbf{Output of this stage:} ranked important sentences and target-worthy content units.
        \item \textbf{What happens (technical):} 
          \begin{enumerate}
            \item sentences are segmented and parsed. Paper do not specify the exact NLP library or parser(like spaCy, NLTK, or OpenNLP). Use AI to check if there is supplementary code / GitHub repo for MCQGen.
            \item each sentence is scored using TF--IDF relevance weighting,
            \item key phrases are extracted using RAKE (Rapid Automatic Keyword Extraction),
            \item semantic similarity models (Sentence-BERT embeddings) are used to check conceptual importance,
            \item Named Entity Recognition (NER) is applied to detect entities, concepts, and definition-bearing elements,
            \item a sentence-importance ranking algorithm selects top candidate sentences for question generation.
          \end{enumerate}
        \item \textbf{Tools used:} TF--IDF vectorizer, RAKE keyword extractor, Sentence-BERT (SBERT) embeddings, spaCy or similar NER models, sentence scoring and ranking algorithms.
      \end{enumerate}
    
    \clearpage

    \section{Stage 4 - Content Selection \& Target Spotting (Technical Version)}
      \begin{enumerate}
        \item \textbf{Input to this stage:} ranked important sentences and extracted key phrases from Stage 3.
        \item \textbf{Output of this stage:} a finalized target concept or fact (the “focus point”) for MCQ generation.
        \item \textbf{What happens (technical):}
        \begin{enumerate}
          \item apply semantic role labeling (SRL) to identify the predicate–argument structure of each important sentence,
          \item detect candidate answer-bearing units (entities, definitions, numerical values, named concepts),
          \item score each candidate target using:
            \begin{enumerate}
              \item TF--IDF relevance,
              \item SBERT semantic centrality,
              \item NER type importance,
              \item frequency and uniqueness within the text,
            \end{enumerate}
          \item eliminate targets that are too vague, too broad, or not factually testable,
          \item select one high-value target concept or fact that can be used to generate a clear MCQ.
        \end{enumerate}
        \item \textbf{Tools used:} semantic role labeling (SRL) models, sentence embeddings (SBERT), TF--IDF scoring, spaCy NER, rule-based target selection filters.
      \end{enumerate}

    \clearpage

    \section{Stage 5 — Prompt Construction for the LLM }
      \begin{enumerate}
        \item \textbf{Input to this stage:} selected target concept/fact and its source sentence from Stage 4.
        \item \textbf{Output of this stage:} a structured, instruction-based prompt ready for the LLM.
        \item \textbf{What happens (technical):}
        \begin{enumerate}
          \item construct an instruction template specifying:
            \begin{enumerate}
              \item task type (MCQ generation),
              \item number of options,
              \item requirement for one correct answer + distractors,
              \item context sentence or supporting text,
              \item clarity, grade level, and cognitive level constraints,
            \end{enumerate}
          \item embed the target concept into the prompt as the “answer source,”
          \item add constraints to ensure:
            \begin{enumerate}
              \item distractors are plausible but clearly incorrect,
              \item the MCQ aligns with the target concept,
              \item no hallucination beyond provided text,
            \end{enumerate}
          \item convert all components into a formatted prompt string suitable for the LLM.
        \end{enumerate}
        \item \textbf{Tools used:} predefined prompt templates, string-formatting scripts, rule-based constraint insertion, prompt validation heuristics.
      \end{enumerate}
    
    \section{Stage 6 — LLM Question Generation }
      \begin{enumerate}
        \item The system uses the \textbf{Google Gemini-Pro 1.0 LLM} (as explicitly stated in the paper) to generate MCQs.
        \item The LLM receives the structured prompt constructed in Stage~5, including:
          \begin{enumerate}
              \item the cleaned and chunked input text,
              \item desired number of questions,
              \item difficulty level,
              \item required cognitive level (Bloom’s),
              \item formatting constraints,
              \item distractor quality guidelines,
              \item output schema for MCQs.
          \end{enumerate}
        
        \clearpage

        \item The LLM is instructed to produce the following for each MCQ:
          \begin{enumerate}
              \item a stem,
              \item one correct answer,
              \item three distractors,
              \item explanation (rationale),
              \item metadata tags (difficulty, Bloom level).
          \end{enumerate}
    
        \item The LLM outputs questions in the enforced \textbf{JSON schema} specified by the authors:
          \begin{enumerate}
              \item \texttt{"question\_text": ...}
              \item \texttt{"options": [...]} 
              \item \texttt{"correct\_answer": ...}
              \item \texttt{"distractors": [...]} 
              \item \texttt{"explanation": ...}
              \item \texttt{"metadata": \{ ... \}}
          \end{enumerate}
    
        \item The authors explicitly emphasize:
          \begin{enumerate}
              \item \textbf{Consistency}: Every generated MCQ must follow the schema without deviation.
              \item \textbf{Content Fidelity}: Questions must be strictly derived from the provided text chunk.
              \item \textbf{Distractor Quality}: Distractors must be plausible but incorrect, and not overlap with correct answer.
              \item \textbf{Difficulty Alignment}: The LLM must adjust phrasing and distractor complexity according to assigned difficulty.
          \end{enumerate}
    
        \item The system relies entirely on the LLM for:
          \begin{enumerate}
              \item question formation,
              \item linguistic validity,
              \item distractor semantics,
              \item explanation generation.
          \end{enumerate}
      \end{enumerate}
    
    \clearpage
    \section{Stage 7 — Post-Generation Evaluation }
      \begin{enumerate}
        \item The authors apply an \textbf{automated rule-based evaluation} to verify the structural and semantic correctness of the LLM-generated MCQs.
        \item The system checks for \textbf{JSON schema compliance}.  
          \begin{enumerate}
              \item All required fields must be present.
              \item No fields may be empty.
              \item Options must include exactly one correct answer and the required number of distractors.
          \end{enumerate}
        \item The system performs \textbf{content validation}:
          \begin{enumerate}
              \item Each MCQ must align with the information present in the corresponding text chunk.
              \item The stem must not introduce extraneous or hallucinated content.
          \end{enumerate}
        \item The authors implement \textbf{duplicate detection}:
          \begin{enumerate}
              \item MCQs with repeating stems or overlapping answer options are removed.
          \end{enumerate}
        \item The authors conduct a \textbf{difficulty-level verification}:
          \begin{enumerate}
              \item Generated questions must match the difficulty level and Bloom category specified during prompt construction.
          \end{enumerate}
        \item The system applies \textbf{linguistic and syntactic correctness checks}:
          \begin{enumerate}
              \item Grammar,
              \item readability,
              \item clarity,
              \item absence of ambiguous phrasing.
          \end{enumerate}
        \item The authors mention \textbf{LLM error detection rules} to catch:
          \begin{enumerate}
              \item malformed questions,
              \item contradictory options,
              \item distractors overlapping with the correct answer,
              \item incorrectly formatted outputs.
          \end{enumerate}
        
        \clearpage

        \item MCQs that fail any of the above checks are marked for \textbf{regeneration}.
          \begin{enumerate}
            \item The authors apply an \textbf{automated rule-based evaluation} to verify the structural and semantic correctness of the LLM-generated MCQs.
            \item The system checks for \textbf{JSON schema compliance}.  
              \begin{enumerate}
                  \item All required fields must be present.
                  \item No fields may be empty.
                  \item Options must include exactly one correct answer and the required number of distractors.
              \end{enumerate}
            \item The system performs \textbf{content validation}:
              \begin{enumerate}
                  \item Each MCQ must align with the information present in the corresponding text chunk.
                  \item The stem must not introduce extraneous or hallucinated content.
              \end{enumerate}
    
            \item The authors implement \textbf{duplicate detection}:
              \begin{enumerate}
                  \item MCQs with repeating stems or overlapping answer options are removed.
              \end{enumerate}
    
            \item The authors conduct a \textbf{difficulty-level verification}:
              \begin{enumerate}
                  \item Generated questions must match the difficulty level and Bloom category specified during prompt construction.
              \end{enumerate}
    
            \item The system applies \textbf{linguistic and syntactic correctness checks}:
              \begin{enumerate}
                  \item Grammar,
                  \item readability,
                  \item clarity,
                  \item absence of ambiguous phrasing.
              \end{enumerate}
            \item The authors mention \textbf{LLM error detection rules} to catch:
              \begin{enumerate}
                  \item malformed questions,
                  \item contradictory options,
                  \item distractors overlapping with the correct answer,
                  \item incorrectly formatted outputs.
              \end{enumerate}
            \item MCQs that fail any of the above checks are marked for \textbf{regeneration}.
        \end{enumerate}
      \end{enumerate}
    
    \clearpage
    \section{Stage 8 — Question Quality Filtering and Scoring}
      \begin{enumerate}
        \item The authors apply an \textbf{automated quality scoring mechanism} to each generated MCQ.
        \item They use a set of \textbf{quality metrics} defined in the system:
          \begin{enumerate}
              \item relevance to the source text,
              \item correctness of the key (correct option),
              \item clarity of the stem,
              \item appropriateness of distractors,
              \item cognitive level alignment with Bloom’s taxonomy,
              \item difficulty-level alignment.
          \end{enumerate}
        \item The system assigns a \textbf{numeric quality score} to every MCQ based on these metrics.
        \item MCQs that fall below a predefined \textbf{quality threshold} are filtered out.
        \item The authors use this filtering step to ensure:
          \begin{enumerate}
              \item only pedagogically valid MCQs are selected,
              \item no low-quality or ambiguous items pass into the final set,
              \item only questions matching user-specified difficulty or Bloom level remain.
          \end{enumerate}
        \item The final output of this stage is a list of \textbf{high-quality, valid MCQs} with associated quality scores.
      \end{enumerate}
      
    \clearpage

    \section{Stage 9 — Final MCQ Assembly and Delivery}
      \begin{enumerate}
        \item The system collects all \textbf{high-quality MCQs} that passed the evaluation and filtering stages.
        \item These MCQs are \textbf{assembled into a structured output format}:
          \begin{enumerate}
              \item JSON format,
              \item compatible with learning platforms,
              \item includes metadata such as Bloom level, difficulty, topic, and source chunk.
          \end{enumerate}
        \item The authors provide an \textbf{exportable output}, which allows the generated MCQs to be:
          \begin{enumerate}
              \item downloaded,
              \item integrated into e-learning tools,
              \item used for assessments or practice tests.
          \end{enumerate}
        
        \item The system also supports \textbf{multiple MCQ types}, and the final output preserves:
          \begin{enumerate}
              \item stem,
              \item options,
              \item correct answer key,
              \item Bloom category,
              \item difficulty level,
              \item reasoning or explanation (when generated).
          \end{enumerate}
        \item The final assembled set represents the end-to-end output of the MCQGen pipeline.
      \end{enumerate}

      \clearpage
  \section{Stage A1 - Input \& Settings }
    \begin{enumerate}
        \item input : 
          \begin{enumerate}
            \item lesson data  
            \item topic/subject 
            \item target grade/age
            \item difficulty level (easy/medium/hard)
            \item Bloom category (if required)
            \item number of MCQs to generate
          \end{enumerate}  
          \item output : A data-structure(may be json) containing clean lesson data and settings.
          \item process :
            \begin{enumerate}
              \item Input lesson text and user settings
              \item Clean text 
              \item Validate input to check if necessary information is present to generate the mcq.
              \item Save information into a data-structure.
            \end{enumerate}
          \item tools
            \begin{enumerate}
              \item Web form to input data
              \item PDF-to-text engine
              \item DOCX parser (paper mentions generic preprocessing, not specific software)
            \end{enumerate}
    \end{enumerate}

  \section{Stage A2 - Text Chunking (Atomic Breakdown) }
    \begin{enumerate}
        \item input : Clean lesson text.
        \item Output : small, clean, logically meaningful chunks 
        \item process :
          \begin{enumerate}
            \item If the text is very short then treat entire text as one chunk.
            \item For long text do segmentation
            \item Group sentences into meaningful chunks so that each chunk covers one idea.
          \end{enumerate} 
          \item tools
            \begin{enumerate}
              \item Sentence segmentation algorithm : create a list of sentences from a big text.
              \item Sentence grouping logic : group sentences into chunks
              \item Semantic similarity check (lightweight) : Validate each chunk covers one idea or sub-topic
            \end{enumerate}
    \end{enumerate}  
  
    \clearpage

  \section{Stage A3 - Tokenize the Text }
    \begin{enumerate}
        \item input : Chunks.
        \item Output : important tokens(words)
        \item process :
          \begin{enumerate}
            \item Split the chunk into individual words (tokens)
            \item Convert all words to lowercase.
            \item Remove punctuation from tokens.
          \end{enumerate} 
          \item tools : Tokenizer
    \end{enumerate}  

  \section{Stage A4 - Remove Stopwords }
    \begin{enumerate}
        \item input : Chunks.
        \item Output : important tokens(words)
        \item process :
          \begin{enumerate}
            \item Split the chunk into individual words (tokens)
            \item Convert all words to lowercase.
            \item Remove punctuation from tokens.
          \end{enumerate} 
          \item tools : Tokenizer
    \end{enumerate}  

  \section{Stage A5 - Remove Stopwords }
    \begin{enumerate}
        \item input : Tokens.
        \item Output : important tokens(words)
        \item process : Remove all stopwords(the, is, at etc)
        \item tools : Stopword filter
    \end{enumerate}    

  \section{Stage A6 - Lemmatize Remaining Tokens }
    \begin{enumerate}
        \item input : Tokens.
        \item Output : important tokens(words)
        \item process : Convert words to base form (running:run), (students:student)
        \item tools : Lemmatizer
    \end{enumerate}

  \section{Stage A7 -  kewords(TF-IDF) extraction }
    \begin{enumerate}
        \item input : Tokens.
        \item Output : TF-IDF Scores of tokens(words)
        \item process : Rank words from highest to lowest importance based on TF-IDF Scores
        \item tools : TF-IDF
    \end{enumerate}

  \section{Stage A8 - phrases(RAKE) extraction }
    \begin{enumerate}
        \item input : chunks.
        \item Output : top-ranked phrases
        \item process : Extract multi-word phrases. Score each phrase based on word co-occurrence. Keep top-ranked phrases.
        \item tools : RAKE algorithm
    \end{enumerate}

  \section{Stage A9 - concepts(TextRank) extraction }
    \begin{enumerate}
        \item input : chunks.
        \item Output : top-scoring concepts
        \item process : Build a graph of words linked by co-occurrence. Apply PageRank-like scoring.
        \item tools : TextRank algorithm
    \end{enumerate}  

  \section{Stage A10 - Compare and Merge Keywords }
    \begin{enumerate}
        \item input : tokens, phrases, concepts.
        \item Output : Filtered keywords list
        \item process : Combine results(TF-IDF, RAKE, TextRank). Remove duplicates and Select top N keyword
        \item tools : Keyword merging and scoring script
    \end{enumerate}  

  \section{Stage A11 - Identify Entities (Optional but mentioned in paper) }
    \begin{enumerate}
        \item input : chunks.
        \item Output : entities to be added to concept list
        \item process : Identify people, places, dates, scientific terms
        \item tools : NER module
    \end{enumerate} 

  \section{Stage A12 - Topic Modeling (Chunk-Level) }
    \begin{enumerate}
        \item input : chunks.
        \item Output : most relevant topic terms
        \item process : Detect hidden topics within the chunk and Add them to concept list.
        \item tools : LDA (Latent Dirichlet Allocation) or similar method, Topic modeling engine
    \end{enumerate} 

  \section{Stage A13 - Semantic Filtering }
    \begin{enumerate}
        \item input : keywords.
        \item Output : relevant keywords
        \item process : Compare extracted keywords with the original chunk. Remove - overly general words, out-of-context words, statistically irrelevant words.
        \item tools : Semantic similarity / relevance scoring
    \end{enumerate} 

  \section{Stage A14 - Final list of keywords }
    \begin{enumerate}
        \item input : keywords, multi word phrases, topic terms, named entities.
        \item Output : final concept list
        \item process : Extract top rated concepts.
        \item tools : Not known
    \end{enumerate}    

    \section{ideas}
      \begin{enumerate}
        \item generate flow charts
        \item generate questions based on flow charts
      \end{enumerate}

    \section{Prompt to understand paper}
      \begin{enumerate}
        \item What the pipeline stages which converts input to final output. Give a step-by-step narrative explanation of each stage (like how data flows and decisions happen)? your answer should be aimed at a beginner school student. Just write the names of the stages we will discuss each stage later.
        \item for each stage write the algorithm that converts input to the output.
        \item write all the tools used in the order they have been used in the pipe-line.
      \end{enumerate}

\end{document}