\documentclass[../../../../main.tex]{subfiles}
\begin{document}


% \input{\subfix{images/Embedding_Models.tex}}
% \input{\subfix{tables/context_window.tex}}

\chapter{MCQGen: A Large Language Model-Driven MCQ Generator for Personalized Learning} 
	
	\section{Information }
	
	\begin{enumerate}
		\item \cite{Hang2024}
    \item IEEE Access , DOI : 10.1109/ACCESS.2024.3420709
		\item \url{https://drive.google.com/file/d/1KkfsKJhbx3S2hLeA6iMZsXpkQdwaRPMb/view?usp=drive_link}
	\end{enumerate}

	\section{Summarize this paper in aimed at a beginner school student. Cover these aspects: the problem the paper addresses, why this problem matters, what the authors did to solve it, how they tested or evaluated their work, and the main result or finding.}
	
	\begin{enumerate}
    \item They built a system called MCQGen to make MCQ (Multiple Choice Questions) for tests and practice.
    \item personalized learning, where each student gets questions that fit their understanding.
  \end{enumerate}

  \clearpage

  \section{What is the the initial input and final output of the work propesed by this paper. aimed at a beginner school student.}
	
	\begin{enumerate}
    \item Input : A paragraph or text that the questions should be based on.
    \item Output: MCQ questions
    \begin{enumerate}
      \item question stem
      \item correct answer
      \item three or four wrong options (distractors)
      \item Difficulty level
      \item Bloom’s Taxonomy level
    \end{enumerate}
  \end{enumerate}

  \section{What the pipeline stages which converts input to final output. Give a step-by-step narrative explanation of each stage (like how data flows and decisions happen)? your answer should be aimed at a beginner school student. Just write the names of the stages we will discuss each stage later.}
  \begin{enumerate}
    \item Input text collection
    \item Preprocessing / cleaning
    \item Important-sentence / concept extraction
    \item Content selection and target spotting
    \item Prompt construction for the LLM
    \item LLM generation (question + correct answer + distractors)
    \item Postprocessing and automatic quality checks
    \item Difficulty estimation and Bloom’s level classification
    \item Human review / validation (optional)
    \item Formatting and final output delivery
  \end{enumerate}

  \clearpage
  \section{Make a table with 5 columns: stage name, input to this stage, description, tools used, outputs of this stage. your answer should be aimed at a beginner school student.write latex code. the table should not be truncated in page. all collumns should be visibel.}
    \input{\subfix{table_02.tex}}

  \clearpage
  \section{Stage 1 - Input \& Settings }
  \begin{enumerate}
    \item \textbf{Input to this stage:} educational text and simple settings (topic, difficulty level, grade, number of questions).
    \item \textbf{Output of this stage:} a set of clean, readable text chunks.
    \item \textbf{What happens (simple):} System breaks input text into clean small chunks if the text is large.
    \item \textbf{Tools used:} A web form or file upload in the system; no heavy AI yet.
  \end{enumerate}


  \section{Stage 2 - Preprocessing \& Text Cleaning}
    \begin{enumerate}
      \item \textbf{Input to this stage:} clean text chunks from Stage 1.
      \item \textbf{Output of this stage:} well-formatted text ready for sentence extraction.
      \item \textbf{What happens (simple):} system removes extra spaces, strange symbols, repeated lines, and makes sure sentences end properly.
      \item \textbf{Tools used:} basic text-cleaning scripts (no AI), simple Python string functions.
    \end{enumerate}

    \section{Stage 3 - Important Sentence Extraction}
      \begin{enumerate}
        \item \textbf{Input to this stage:} cleaned text chunks from Stage 2.
        \item \textbf{Output of this stage:} ranked important sentences and target-worthy content units.
        \item \textbf{What happens (technical):} 
          \begin{enumerate}
            \item sentences are segmented and parsed. Paper do not specify the exact NLP library or parser(like spaCy, NLTK, or OpenNLP). Use AI to check if there is supplementary code / GitHub repo for MCQGen.
            \item each sentence is scored using TF--IDF relevance weighting,
            \item key phrases are extracted using RAKE (Rapid Automatic Keyword Extraction),
            \item semantic similarity models (Sentence-BERT embeddings) are used to check conceptual importance,
            \item Named Entity Recognition (NER) is applied to detect entities, concepts, and definition-bearing elements,
            \item a sentence-importance ranking algorithm selects top candidate sentences for question generation.
          \end{enumerate}
        \item \textbf{Tools used:} TF--IDF vectorizer, RAKE keyword extractor, Sentence-BERT (SBERT) embeddings, spaCy or similar NER models, sentence scoring and ranking algorithms.
      \end{enumerate}
    
    \clearpage

    \section{Stage 4 - Content Selection \& Target Spotting (Technical Version)}
      \begin{enumerate}
        \item \textbf{Input to this stage:} ranked important sentences and extracted key phrases from Stage 3.
        \item \textbf{Output of this stage:} a finalized target concept or fact (the “focus point”) for MCQ generation.
        \item \textbf{What happens (technical):}
        \begin{enumerate}
          \item apply semantic role labeling (SRL) to identify the predicate–argument structure of each important sentence,
          \item detect candidate answer-bearing units (entities, definitions, numerical values, named concepts),
          \item score each candidate target using:
            \begin{enumerate}
              \item TF--IDF relevance,
              \item SBERT semantic centrality,
              \item NER type importance,
              \item frequency and uniqueness within the text,
            \end{enumerate}
          \item eliminate targets that are too vague, too broad, or not factually testable,
          \item select one high-value target concept or fact that can be used to generate a clear MCQ.
        \end{enumerate}
        \item \textbf{Tools used:} semantic role labeling (SRL) models, sentence embeddings (SBERT), TF--IDF scoring, spaCy NER, rule-based target selection filters.
      \end{enumerate}

    \clearpage

    \section{Stage 5 — Prompt Construction for the LLM }
      \begin{enumerate}
        \item \textbf{Input to this stage:} selected target concept/fact and its source sentence from Stage 4.
        \item \textbf{Output of this stage:} a structured, instruction-based prompt ready for the LLM.
        \item \textbf{What happens (technical):}
        \begin{enumerate}
          \item construct an instruction template specifying:
            \begin{enumerate}
              \item task type (MCQ generation),
              \item number of options,
              \item requirement for one correct answer + distractors,
              \item context sentence or supporting text,
              \item clarity, grade level, and cognitive level constraints,
            \end{enumerate}
          \item embed the target concept into the prompt as the “answer source,”
          \item add constraints to ensure:
            \begin{enumerate}
              \item distractors are plausible but clearly incorrect,
              \item the MCQ aligns with the target concept,
              \item no hallucination beyond provided text,
            \end{enumerate}
          \item convert all components into a formatted prompt string suitable for the LLM.
        \end{enumerate}
        \item \textbf{Tools used:} predefined prompt templates, string-formatting scripts, rule-based constraint insertion, prompt validation heuristics.
      \end{enumerate}
    
    \section{Stage 6 — LLM Question Generation }
      \begin{enumerate}
        \item The system uses the \textbf{Google Gemini-Pro 1.0 LLM} (as explicitly stated in the paper) to generate MCQs.
        \item The LLM receives the structured prompt constructed in Stage~5, including:
          \begin{enumerate}
              \item the cleaned and chunked input text,
              \item desired number of questions,
              \item difficulty level,
              \item required cognitive level (Bloom’s),
              \item formatting constraints,
              \item distractor quality guidelines,
              \item output schema for MCQs.
          \end{enumerate}
        
        \clearpage

        \item The LLM is instructed to produce the following for each MCQ:
          \begin{enumerate}
              \item a stem,
              \item one correct answer,
              \item three distractors,
              \item explanation (rationale),
              \item metadata tags (difficulty, Bloom level).
          \end{enumerate}
    
        \item The LLM outputs questions in the enforced \textbf{JSON schema} specified by the authors:
          \begin{enumerate}
              \item \texttt{"question\_text": ...}
              \item \texttt{"options": [...]} 
              \item \texttt{"correct\_answer": ...}
              \item \texttt{"distractors": [...]} 
              \item \texttt{"explanation": ...}
              \item \texttt{"metadata": \{ ... \}}
          \end{enumerate}
    
        \item The authors explicitly emphasize:
          \begin{enumerate}
              \item \textbf{Consistency}: Every generated MCQ must follow the schema without deviation.
              \item \textbf{Content Fidelity}: Questions must be strictly derived from the provided text chunk.
              \item \textbf{Distractor Quality}: Distractors must be plausible but incorrect, and not overlap with correct answer.
              \item \textbf{Difficulty Alignment}: The LLM must adjust phrasing and distractor complexity according to assigned difficulty.
          \end{enumerate}
    
        \item The system relies entirely on the LLM for:
          \begin{enumerate}
              \item question formation,
              \item linguistic validity,
              \item distractor semantics,
              \item explanation generation.
          \end{enumerate}
      \end{enumerate}
    
    \clearpage
    \section{Stage 7 — Post-Generation Evaluation }
      \begin{enumerate}
        \item The authors apply an \textbf{automated rule-based evaluation} to verify the structural and semantic correctness of the LLM-generated MCQs.
        \item The system checks for \textbf{JSON schema compliance}.  
          \begin{enumerate}
              \item All required fields must be present.
              \item No fields may be empty.
              \item Options must include exactly one correct answer and the required number of distractors.
          \end{enumerate}
        \item The system performs \textbf{content validation}:
          \begin{enumerate}
              \item Each MCQ must align with the information present in the corresponding text chunk.
              \item The stem must not introduce extraneous or hallucinated content.
          \end{enumerate}
        \item The authors implement \textbf{duplicate detection}:
          \begin{enumerate}
              \item MCQs with repeating stems or overlapping answer options are removed.
          \end{enumerate}
        \item The authors conduct a \textbf{difficulty-level verification}:
          \begin{enumerate}
              \item Generated questions must match the difficulty level and Bloom category specified during prompt construction.
          \end{enumerate}
        \item The system applies \textbf{linguistic and syntactic correctness checks}:
          \begin{enumerate}
              \item Grammar,
              \item readability,
              \item clarity,
              \item absence of ambiguous phrasing.
          \end{enumerate}
        \item The authors mention \textbf{LLM error detection rules} to catch:
          \begin{enumerate}
              \item malformed questions,
              \item contradictory options,
              \item distractors overlapping with the correct answer,
              \item incorrectly formatted outputs.
          \end{enumerate}
        
        \clearpage

        \item MCQs that fail any of the above checks are marked for \textbf{regeneration}.
          \begin{enumerate}
            \item The authors apply an \textbf{automated rule-based evaluation} to verify the structural and semantic correctness of the LLM-generated MCQs.
            \item The system checks for \textbf{JSON schema compliance}.  
              \begin{enumerate}
                  \item All required fields must be present.
                  \item No fields may be empty.
                  \item Options must include exactly one correct answer and the required number of distractors.
              \end{enumerate}
            \item The system performs \textbf{content validation}:
              \begin{enumerate}
                  \item Each MCQ must align with the information present in the corresponding text chunk.
                  \item The stem must not introduce extraneous or hallucinated content.
              \end{enumerate}
    
            \item The authors implement \textbf{duplicate detection}:
              \begin{enumerate}
                  \item MCQs with repeating stems or overlapping answer options are removed.
              \end{enumerate}
    
            \item The authors conduct a \textbf{difficulty-level verification}:
              \begin{enumerate}
                  \item Generated questions must match the difficulty level and Bloom category specified during prompt construction.
              \end{enumerate}
    
            \item The system applies \textbf{linguistic and syntactic correctness checks}:
              \begin{enumerate}
                  \item Grammar,
                  \item readability,
                  \item clarity,
                  \item absence of ambiguous phrasing.
              \end{enumerate}
            \item The authors mention \textbf{LLM error detection rules} to catch:
              \begin{enumerate}
                  \item malformed questions,
                  \item contradictory options,
                  \item distractors overlapping with the correct answer,
                  \item incorrectly formatted outputs.
              \end{enumerate}
            \item MCQs that fail any of the above checks are marked for \textbf{regeneration}.
        \end{enumerate}
      \end{enumerate}
    
    \clearpage
    \section{Stage 8 — Question Quality Filtering and Scoring}
      \begin{enumerate}
        \item The authors apply an \textbf{automated quality scoring mechanism} to each generated MCQ.
        \item They use a set of \textbf{quality metrics} defined in the system:
          \begin{enumerate}
              \item relevance to the source text,
              \item correctness of the key (correct option),
              \item clarity of the stem,
              \item appropriateness of distractors,
              \item cognitive level alignment with Bloom’s taxonomy,
              \item difficulty-level alignment.
          \end{enumerate}
        \item The system assigns a \textbf{numeric quality score} to every MCQ based on these metrics.
        \item MCQs that fall below a predefined \textbf{quality threshold} are filtered out.
        \item The authors use this filtering step to ensure:
          \begin{enumerate}
              \item only pedagogically valid MCQs are selected,
              \item no low-quality or ambiguous items pass into the final set,
              \item only questions matching user-specified difficulty or Bloom level remain.
          \end{enumerate}
        \item The final output of this stage is a list of \textbf{high-quality, valid MCQs} with associated quality scores.
      \end{enumerate}
      
    \clearpage

    \section{Stage 9 — Final MCQ Assembly and Delivery}
      \begin{enumerate}
        \item The system collects all \textbf{high-quality MCQs} that passed the evaluation and filtering stages.
        \item These MCQs are \textbf{assembled into a structured output format}:
          \begin{enumerate}
              \item JSON format,
              \item compatible with learning platforms,
              \item includes metadata such as Bloom level, difficulty, topic, and source chunk.
          \end{enumerate}
        \item The authors provide an \textbf{exportable output}, which allows the generated MCQs to be:
          \begin{enumerate}
              \item downloaded,
              \item integrated into e-learning tools,
              \item used for assessments or practice tests.
          \end{enumerate}
        
        \item The system also supports \textbf{multiple MCQ types}, and the final output preserves:
          \begin{enumerate}
              \item stem,
              \item options,
              \item correct answer key,
              \item Bloom category,
              \item difficulty level,
              \item reasoning or explanation (when generated).
          \end{enumerate}
        \item The final assembled set represents the end-to-end output of the MCQGen pipeline.
      \end{enumerate}

      \clearpage
  \section{Stage A1 - Input \& Settings }
    \begin{enumerate}
        \item input : 
          \begin{enumerate}
            \item lesson data  
            \item topic/subject 
            \item target grade/age
            \item difficulty level (easy/medium/hard)
            \item Bloom category (if required)
            \item number of MCQs to generate
          \end{enumerate}  
          \item output : A data-structure(may be json) containing clean lesson data and settings.
          \item process :
            \begin{enumerate}
              \item Input lesson text and user settings
              \item Clean text 
              \item Validate input to check if necessary information is present to generate the mcq.
              \item Save information into a data-structure.
            \end{enumerate}
          \item tools
            \begin{enumerate}
              \item Web form to input data
              \item PDF-to-text engine
              \item DOCX parser (paper mentions generic preprocessing, not specific software)
            \end{enumerate}
    \end{enumerate}

  \section{Stage A2 - Text Chunking (Atomic Breakdown) }
    \begin{enumerate}
        \item input : Clean lesson text.
        \item Output : small, clean, logically meaningful chunks 
        \item process :
          \begin{enumerate}
            \item If the text is very short then treat entire text as one chunk.
            \item For long text do segmentation
            \item Group sentences into meaningful chunks so that each chunk covers one idea.
          \end{enumerate} 
          \item tools
            \begin{enumerate}
              \item Sentence segmentation algorithm : create a list of sentences from a big text.
              \item Sentence grouping logic : group sentences into chunks
              \item Semantic similarity check (lightweight) : Validate each chunk covers one idea or sub-topic
            \end{enumerate}
    \end{enumerate}  
  
    \clearpage

  \section{Stage A3 - Tokenize the Text }
    \begin{enumerate}
        \item input : Chunks.
        \item Output : important tokens(words)
        \item process :
          \begin{enumerate}
            \item Split the chunk into individual words (tokens)
            \item Convert all words to lowercase.
            \item Remove punctuation from tokens.
          \end{enumerate} 
          \item tools : Tokenizer
    \end{enumerate}  

  \section{Stage A5 - Remove Stopwords }
    \begin{enumerate}
        \item input : Tokens.
        \item Output : important tokens(words)
        \item process : Remove all stopwords(the, is, at etc)
        \item tools : Stopword filter
    \end{enumerate}    

  \section{Stage A6 - Lemmatize Remaining Tokens }
    \begin{enumerate}
        \item input : Tokens.
        \item Output : important tokens(words)
        \item process : Convert words to base form (running:run), (students:student)
        \item tools : Lemmatizer
    \end{enumerate}

  \section{Stage A7 -  kewords(TF-IDF) extraction }
    \begin{enumerate}
        \item input : Tokens.
        \item Output : TF-IDF Scores of tokens(words)
        \item process : Rank words from highest to lowest importance based on TF-IDF Scores
        \item tools : TF-IDF
    \end{enumerate}

  \section{Stage A8 - phrases(RAKE) extraction }
    \begin{enumerate}
        \item input : chunks.
        \item Output : top-ranked phrases
        \item process : Extract multi-word phrases. Score each phrase based on word co-occurrence. Keep top-ranked phrases.
        \item tools : RAKE algorithm
    \end{enumerate}

  \section{Stage A9 - concepts(TextRank) extraction }
    \begin{enumerate}
        \item input : chunks.
        \item Output : top-scoring concepts
        \item process : Build a graph of words linked by co-occurrence. Apply PageRank-like scoring.
        \item tools : TextRank algorithm
    \end{enumerate}  

  \section{Stage A10 - Compare and Merge Keywords }
    \begin{enumerate}
        \item input : tokens, phrases, concepts.
        \item Output : Filtered keywords list
        \item process : Combine results(TF-IDF, RAKE, TextRank). Remove duplicates and Select top N keyword
        \item tools : Keyword merging and scoring script
    \end{enumerate}  

  \section{Stage A11 - Identify Entities (Optional but mentioned in paper) }
    \begin{enumerate}
        \item input : chunks.
        \item Output : entities to be added to concept list
        \item process : Identify people, places, dates, scientific terms
        \item tools : NER module
    \end{enumerate} 

  \section{Stage A12 - Topic Modeling (Chunk-Level) }
    \begin{enumerate}
        \item input : chunks.
        \item Output : most relevant topic terms
        \item process : Detect hidden topics within the chunk and Add them to concept list.
        \item tools : LDA (Latent Dirichlet Allocation) or similar method, Topic modeling engine
    \end{enumerate} 

  \section{Stage A13 - Semantic Filtering }
    \begin{enumerate}
        \item input : keywords.
        \item Output : relevant keywords
        \item process : Compare extracted keywords with the original chunk. Remove - overly general words, out-of-context words, statistically irrelevant words.
        \item tools : Semantic similarity / relevance scoring
    \end{enumerate} 

  \section{Stage A14 - Final list of keywords }
    \begin{enumerate}
        \item input : keywords, multi word phrases, topic terms, named entities.
        \item Output : final concept list
        \item process : Extract top rated concepts.
        \item tools : Not known
    \end{enumerate} 
    \section{Stage A15 - Generate Concept Embeddings }
      \begin{enumerate}
          \item input : final concept list
          \item Output : vector for each concept.
          \item process : Convert each concept into a vector using embeddings (word embeddings or sentence embeddings).
          \item tools : pretrained embedding model (paper uses embedding-based analysis).
      \end{enumerate}
  
    \section{Stage A16 - Build Semantic Similarity Matrix }
      \begin{enumerate}
          \item input : vector for each concept
          \item Output : similarity matrix.
          \item process : Compute similarity scores (cosine similarity).
          \item tools : similarity module
      \end{enumerate}
  
    \section{Stage A17 - Identify Concepts That Are Too Similar }
      \begin{enumerate}
          \item input : similarity matrix.
          \item Output : list of terms with extremely high similarity.
          \item process : based of a threshold mark the terms which are extremely high similar. Avoid them (they might accidentally be second correct answers).
          \item tools : threshold-based similarity filter
      \end{enumerate}
  
    \section{Stage A18 - Identify Concepts That Are Too Dissimilar }
      \begin{enumerate}
          \item input : similarity matrix.
          \item Output : list of terms with extremely high dissimilarity.
          \item process : based of a threshold mark the terms which are high dissimilarity. Avoid them (unsuitable as distractors).
          \item tools : dissimilarity filter
      \end{enumerate} 
  
    \section{Stage A19 - Extract Lexical Relations }
      \begin{enumerate}
          \item input : final concept list(check this once).
          \item Output : related lexical nodes
          \item process : Build a lexical relation graph. Check synonyms (avoid — too close). Check hypernyms/hyponyms (general/specific terms). Extract related but incorrect terms.
          \item tools : WordNet or similar lexical database (paper mentions semantic/lexical analysis)
      \end{enumerate}
  
    \section{Stage A20 - Generate Candidate Distractor Pool }
      \begin{enumerate}
          \item input : semantically close but not identical concepts, related lexical nodes, related entities.
          \item Output : list of raw distractor candidates
          \item process : Merge
          \item tools : candidate pool builder
      \end{enumerate}
  
    \section{Stage A21 - Apply Rule-Based Filtering to remove undesirable distractos }
      \begin{enumerate}
          \item input : list of raw distractor candidates
          \item Output : filtered list of distractor candidates
          \item process : Remove words that appear inside the chunk’s answer sentence. Remove negations (e.g., "not", "never"). Remove words that change grammatical class (noun needed : remove verbs). Remove rare or overly technical terms inappropriate for grade level.
          \item tools : rule-based filter (paper clearly mentions rule-based checks).
      \end{enumerate}
  
    \section{Stage A22 - Apply Difficulty-Level Constraints }
      \begin{enumerate}
          \item input : filtered list of distractor candidates
          \item Output : filtered list of distractor candidates
          \item process : If difficulty is easy then distractors should be simpler and more visually different. If difficulty is hard then distractors should be conceptually closer.
          \item tools : difficulty-based scoring.
      \end{enumerate}
  
    \section{Stage A23 - Apply Blooms Taxonomy Constraints }
      \begin{enumerate}
          \item input : filtered list of distractor candidates
          \item Output : filtered list of distractor candidates
          \item process : Remember then distractors based on surface similarity.. Understand/Apply then distractors must require deeper reasoning. Remove distractors inconsistent with targeted Bloom category.
          \item tools : Bloom mapping rules.
      \end{enumerate}
  
    \section{Stage A24 - Score Remaining Candidate Distractors }
      \begin{enumerate}
          \item input : filtered list of distractor candidates, semantic similarity, lexical appropriateness, difficulty match, Bloom-level alignment.
          \item Output : top-ranked distractor candidates for each target concept
          \item process : numeric score based on semantic similarity, lexical appropriateness, difficulty match, Bloom-level alignment
          \item tools : scoring module described in paper.
      \end{enumerate}
  
    \section{Stage A25 - Build prompt }
      \begin{enumerate}
          \item input :  Prompt Template, Output Schema Format(json), chunk text, target concept, distractors candidates, difficulty, blooms level, instructions
          \item Output : complete, structured prompt(Json) for the LLM, filled with details
          \item process : Produce a complete, structured prompt for the LLM. Insert details in the Prompt Template(json). Add instructions such as 
            \begin{enumerate} 
              \item Generate exactly one MCQ
              \item Use the provided concept as the correct answer
              \item Use distractor candidates but do not repeat the correct answer
              \item Output format must follow JSON / structured text
              \item specify Output Schema Format(json) 
            \end{enumerate}
          \item tools : prompt builder, prompt template manager, handler to insert data, 
      \end{enumerate}
  
    \section{Stage A26 - Generate MCQs using LLM }
      \begin{enumerate}
          \item input : prompt, LLM API, inference parameters. 
          \item Output : Set of MCQs
          \item process : 
          \begin{enumerate}
            \item Set values for temperature, top-k, top-p, max tokens, repetition penalty, deterministic or creative mode etc. 
            \item Group constructed prompts into batches to fits within token constraints.
            \item Submit prompt batches to the LLM via API
            \item Parse the LLM response into structured MCQ format
          \end{enumerate}
          \item tools : LLM API, HuggingFace transformers, HTTP API
      \end{enumerate}
  
    \section{Stage A27 - Linguistic Quality Check }
      \begin{enumerate}
          \item input : Set of MCQs
          \item Output : MCQ with grammar issues
          \item process : various Quality Check, Measure readability scores
          \item tools : spaCy, LanguageTool, Grammarly API, Flesch–Kincaid
      \end{enumerate}
  
    \section{Stage A28 -  Content Correctness Verification }
      \begin{enumerate}
          \item input : Set of MCQs
          \item Output : MCQ with issues
          \item process : 
          \begin{enumerate}
            \item Ensure stems provide sufficient context.
            \item Evaluate alignment with extracted key concepts.
            \item Compare correct answer with ground truth extracted from chunks, concepts
            \item Use semantic similarity scoring (cosine similarity via sentence-transformers).
          \end{enumerate}
          \item tools : not known
      \end{enumerate}          
  
      \section{Stage A29 - Distractor Plausibility }
      \begin{enumerate}
          \item input : Set of MCQs
          \item Output : MCQ with issues
          \item process : 
          \begin{enumerate}
            \item Score semantic distance from correct answer
            \item Reject distractors that are implausible or correct
            \item Reject distractors that are too close to the correct answer.
          \end{enumerate}
          \item tools : not known
      \end{enumerate}          

  
      \section{Stage A30 -  Final Decision}
      \begin{enumerate}
          \item input : Set of MCQs
          \item Output : Refined set of MCQs
          \item process :  Accept if
          \begin{enumerate}
            \item passes schema
            \item high correctness
            \item relevant
          \end{enumerate}
          \item tools : not known
      \end{enumerate}          
      
  
      \section{Stage A31 - Final list of MCQsStage 9 with the same atomic-level depth }
      \begin{enumerate}
          \item Filter by difficulty level
          \item Filter by Bloom's taxonomy level
          \item Filter by grade-level relevance.
          \item Keep only MCQs that align with topic and key concepts of the lesson
          \item score the mcq and select Top-K Questions 
      \end{enumerate}          

  \section{Imp points}
    \begin{enumerate}
        \item Ask chatgpt what steps(Algorithm, stages) it uses to create a mcq question from lesson text.
        \item techinque of this paper is to create a prompt for the LLM to get a mcq. This prompt will contain a concept and its destractors.
        \item Improve mcqGen for multiple correct choices, currently it has only one correct answers.
    \end{enumerate} 


    \section{Prompt to understand paper}
      \begin{enumerate}
        \item We will create the pipeline stages one by one. Start with the first stage. Discuss.
      \end{enumerate}

      \section{Ideas}
        \begin{enumerate}
          \item Technique to create distractors for mcq. 
          \item Use the technique presented in MCQ gen paper to create(generate) explanation/tutorial of a research paper. 
          \item Improve mcqGen for multiple correct choices, currently it has only one correct answers.
        \end{enumerate}

    \section{Tools List}
    \begin{enumerate}
      \item PDF-to-text engine
      \item DOCX parser (paper mentions generic preprocessing, not specific software)
      \item Sentence segmentation algorithm : create a list of sentences from a big text.
      \item Sentence grouping logic : group sentences into chunks
      \item Semantic similarity check (lightweight) : Validate each chunk covers one idea or sub-topic Tokenizer
      \item Tokenizer
      \item Stopword filter
      \item Lemmatizer
      \item TF-IDF
      \item RAKE algorithm
      \item TextRank algorithm
      \item Keyword merging and scoring script
      \item NER module
      \item LDA (Latent Dirichlet Allocation) or similar method, Topic modeling engine
      \item Semantic similarity / relevance scoring
    \end{enumerate}

\end{document}