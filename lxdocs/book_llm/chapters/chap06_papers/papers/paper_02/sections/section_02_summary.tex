\section{Summary}

\begin{enumerate}
    \item Input
        \begin{enumerate}
            \item Problem description (Python exercise)
            \item Python Code (For that problem)
        \end{enumerate}
    \item Output
    \begin{enumerate}
        \item Result (pass/fail on tests + quality checks)
        \item Review comments: Each comment is Learner-friendly that highlights bad lines and  explain how to improve the code.
    \end{enumerate}
    \item This paper proposes a GPT-4o-based code review system for python that provides accurate feedback(comments).
    \item The system tells students what is wrong and which lines they should fix
    \item Use of LLMs in class allows AI-assisted cheating. If novice learners use ChatGPT to easily generate code to solve problems, their learning effectiveness may decrease.
    \item The system tells students what is wrong and which lines they should fix, but it does not show the full correct answer. This helps reduce cheating with AI.
    \item It also tries not to waste too many AI tokens, so it is cheaper to run for big classes. 
    \item  introduces a Review Necessity Chain (RNC) to reduce unnecessary feedback and API usage
    \item Research Questions
      \begin{enumerate}
        \item Does the Code Correctness Check Module improve the relative error detection rate compared to a traditional online judge system?
        \item  How semantically similar is the feedback generated by the Code Review Module (CRM) to those produced by human instructors?
        \item Does the pipeline of the CRM effectively reduce unnecessary reviews while retaining essential feedback?
        \item Can the proposed code review system improve learning efficiency and reduce instructorsâ€™ feedback workload in large-scale educational settings?
      \end{enumerate}
    \item Results
      \begin{enumerate}
        \item The results suggest this system can help teachers and make it easier for school students to learn programming.
      \end{enumerate}
      \item They created all prompt templates during the "prompt design phase" (Section III.B). They used their dataset of 93 student submissions to build and refine them. LangChain just helps manage these custom templates
      \item Exercises, student submissions, and instructor solutions were collected from Existing Online Judge (OJ) dataset (Company C's C3 Coding OJ). A total of 93 test data for 27 questions.
\end{enumerate}